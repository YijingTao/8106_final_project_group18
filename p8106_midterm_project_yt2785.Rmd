---
title: "Midterm Project"
author: "Yijing Tao yt2785"
date: '2022-03-16'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(readxl)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(mgcv)
library(earth)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
library(lasso2)
library(vip)
library(summarytools)
library(ISLR)
library(caret)
library(vip)
library(pdp)
library(lime)
library(mlbench)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer) 
library(gplots)
library(jpeg)
```

## import data set (70% to train data while 30% to test data)
```{r}
dat_df_nores = read_csv("./data.csv")%>% 
  data.frame() %>% 
  dplyr::select(-diagnosis,-id)
dat_df_res = read_csv("./data.csv") %>% 
  data.frame() %>% 
  dplyr::select(diagnosis)
dat_df = cbind(dat_df_nores, dat_df_res) %>% 
  data.frame() 

dat_df2 <- model.matrix(diagnosis ~ ., dat_df)[ ,-1]
#assign 70% to the training set and the rest 30% to the test set 
set.seed(2022)
trainRows <- createDataPartition(dat_df$diagnosis, p = .7, list = F)

train <- dat_df[trainRows,]
x1 <- dat_df2[trainRows,]
y1 <- dat_df$diagnosis[trainRows]

# matrix of predictors (glmnet uses input matrix)
test <- dat_df[-trainRows,]
x2 <- dat_df2[-trainRows,]
y2 <- dat_df$diagnosis[-trainRows]

ctrl <- trainControl(method = "cv", number = 10)
```

## visualization
```{r}
dfSummary(dat_df[,-1])
```

```{r, fig.height = 4}
train_M <- train %>% 
  filter(diagnosis == "M")
train_B <- train %>% 
  filter(diagnosis == "B")

mfrow=c(2,30)
boxplot(train_M[,1])
boxplot(train_B[,1])
boxplot(train_M[,2])
boxplot(train_B[,2])
boxplot(train_M[,3])
boxplot(train_B[,3])
boxplot(train_M[,4])
boxplot(train_B[,4])
boxplot(train_M[,5])
boxplot(train_B[,5])
boxplot(train_M[,6])
boxplot(train_B[,6])
boxplot(train_M[,1])
boxplot(train_B[,1])
boxplot(train_M[,1])
boxplot(train_B[,1])
```

## KNN model
```{r}
kGrid <- expand.grid(k = seq(from = 1, to = 20, by = 1))
set.seed(2022)
knn.fit <- train(Life.expectancy ~., 
                data = train, 
                method = "knn",
                trControl = ctrl,
                tuneGrid = kGrid)

ggplot(knn.fit)
knn.fit$bestTune

knn.pred <- predict(knn.fit, newdata = test[,1:18])
# test error
knn_te <- mean((knn.pred - y2)^2)
knn_te
```

## linear model
```{r}
set.seed(2022)
lm.fit <- train(Life.expectancy ~ ., 
                data = train,
                method = "lm",
                trControl = ctrl)

lm.pred <- predict(lm.fit, newdata = test[,1:18])
# test error
lm_te <- mean((lm.pred - y2)^2)
lm_te
```

## Ridge
```{r}
set.seed(2022)
ridge.fit <- train(x = x1, 
                   y = y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(2, -3, length=100))),
                   trControl = ctrl)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune
log(ridge.fit$bestTune)

# coefficients in the final model
coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda)

ridge.pred <- predict(ridge.fit, newdata = x2)
# test error
ridge_te <- mean((ridge.pred - y2)^2)
ridge_te
```

## Lasso
```{r}
set.seed(2022)
lasso.fit <- train(x = x1, 
                   y = y1,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(0, -8, length=100))),
                   trControl = ctrl)

plot(lasso.fit, xTrans = log)

lasso.fit$bestTune
log(lasso.fit$bestTune)

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

lasso.pred <- predict(lasso.fit, newdata = x2)
# test error
lasso_te <- mean((lasso.pred - y2)^2)
lasso_te
```

## Elastic net
```{r}
set.seed(2022)
enet.fit <- train(x = x1,
                  y = y1,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(-3, 1, length = 21), 
                                         lambda = exp(seq(0, -10, length = 50))),
                  trControl = ctrl
                  )

enet.fit$bestTune
log(enet.fit$bestTune)

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
                    superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)

enet.pred <- predict(enet.fit, newdata = x2)
# test error
enet_te <- mean((enet.pred - y2)^2)
enet_te
```

## PCR
```{r}
set.seed(2022)
pcr.fit <- train(x = x1, 
                 y = y1,
                  method = "pcr",
                  tuneGrid = data.frame(ncomp = 1:18),
                  trControl = ctrl,
                 preProcess = c("center", "scale"))

pcr.fit$bestTune

# test error
pcr.pred <- predict(pcr.fit, newdata = x2)
pcr_te <- mean((y2 - pcr.pred)^2)
pcr_te
ggplot(pcr.fit, highlight = TRUE) + theme_bw()
```

## PLS
```{r}
set.seed(2022)
pls.fit <- train(x = x1, 
                 y = y1,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:18),
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

pls.fit$bestTune

#test error
pls.pred <- predict(pls.fit, newdata = x2)
pls_te <- mean((y2 - pls.pred)^2)
pls_te
ggplot(pls.fit, highlight = TRUE)
```

## GAM
```{r}
set.seed(2022)
gam.fit <- train(x = x1, 
                 y = y1,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE, FALSE)),
                 trControl = ctrl)

gam.fit$bestTune

gam.fit$finalModel

#test error
gam.pred <- predict(gam.fit, newdata = x2)
gam_te <- mean((y2 - gam.pred)^2)
gam_te
```

## MARS
```{r}
mars_grid <- expand.grid(degree = 1:5, 
                         nprune = 10:29)

set.seed(2022)
mars.fit <- train(x = x1, 
                  y = y1,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl)

ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel) 

#test error
mars.pred <- predict(mars.fit, newdata = x2)
mars_te <- mean((y2 - mars.pred)^2)
mars_te
```

## Comparing different models
```{r}
model <- c("KNN","LN","RIDGE","LASSO","ENET","PCR","PLS","GAM","MARS")
test_error <- c(knn_te,lm_te,ridge_te,lasso_te,enet_te,pcr_te,pls_te,gam_te,mars_te)
test_error_df <- cbind(model, test_error)
test_error_df <- as.data.frame(test_error_df)
test_error_df

resamp <- resamples(list(KNN = knn.fit,
                         LN = lm.fit,
                         RIDGE = ridge.fit,
                         LASSO = lasso.fit,
                         ENET = enet.fit,
                         PCR = pcr.fit,
                         PLS = pls.fit,
                         GAM = gam.fit,
                         MARS = mars.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```
**MARS is the best fitting model since it has the smallest RMSE.**

```{r}
summary(mars.fit$finalModel)

vip(mars.fit$finalModel)

p1 <- pdp::partial(mars.fit, pred.var = c("Adult.Mortality"), grid.resolution = 10) %>%
  autoplot()
p2 <- pdp::partial(mars.fit, pred.var = c("BMI"), grid.resolution = 10) %>%
  autoplot()
p3 <- pdp::partial(mars.fit, pred.var = c("Diphtheria"), grid.resolution = 10) %>%
  autoplot()
p4 <- pdp::partial(mars.fit, pred.var = c("Income.composition.of.resources"), grid.resolution = 10) %>%
  autoplot()
p5 <- pdp::partial(mars.fit, pred.var = c("infant.deaths"), grid.resolution = 10) %>%
  autoplot()
p6 <- pdp::partial(mars.fit, pred.var = c("HIV.AIDS"), grid.resolution = 10) %>%
  autoplot()
p7 <- pdp::partial(mars.fit, pred.var = c("thinness.5.9.years"), grid.resolution = 10) %>%
  autoplot()

grid.arrange(p1, p2, p3, p4, p5, p6, p7, ncol = 3)

```

## Random forest
```{r}
rf.grid <- expand.grid(mtry = 1:6,
                       splitrule = "variance",
                       min.node.size = 1:6)
set.seed(2022)
rf.fit <- train(Life.expectancy~., train, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)

#test error
rf.pred <- predict(rf.fit, newdata = test[,1:18])
rf_te <- mean((y2 - rf.pred)^2)
rf_te
```

### Variable importance
```{r}
set.seed(2022)
vip(rf.fit, 
    method = "permute", 
    train = train,
    target = "Life.expectancy",
    metric = "RMSE",
    nsim = 10,
    pred_wrapper = predict,
    geom = "boxplot", 
    all_permutations = TRUE,
    mapping = aes_string(fill = "Variable")) 
```

### pdp ?????
```{r, fig.width = 8, fig.height = 4}
pdp1.rf <- rf.fit %>% 
  partial(pred.var = c("CRBI")) %>%
  autoplot(train = train, rug = TRUE) 

pdp2.rf <- rf.fit %>% 
  partial(pred.var = c("CRBI","CAtBat"), chull = TRUE) %>%
  autoplot(train = train, rug = TRUE) 

grid.arrange(pdp1.rf, pdp2.rf, nrow = 1)
```

### Individual conditional expectation (ICE) curves ???
```{r, fig.width = 8, fig.height = 4}
ice1.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = expect_df, alpha = .1) +
  ggtitle("ICE, not centered") 

ice2.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = expect_df, alpha = .1, 
           center = TRUE) +
  ggtitle("ICE, centered") 


grid.arrange(ice1.rf, ice2.rf, nrow = 1)
```

### lime ????
```{r, warning=FALSE, fig.height = 10}
explainer.rf <- lime(train[,1:18], rf.fit)

new_obs <- test[,1:18][1:10,]
explanation.obs <- explain(new_obs,
                           explainer.rf, 
                           n_features = 10)

plot_features(explanation.obs)
plot_explanations(explanation.obs)
```


# K means clustering
```{r}
dat <- scale(expect_df[,-1])
```


```{r, fig.height=3.5}
fviz_nbclust(dat,
             FUNcluster = kmeans,
             method = "silhouette")

km <- kmeans(dat, centers = 2, nstart = 20)
```

```{r}
km_vis <- fviz_cluster(list(data = dat, cluster = km$cluster), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 

km_vis
```

# Hierarchical clustering
```{r}
hc.complete <- hclust(dist(dat), method = "complete")
```

```{r, fig.width=7}
fviz_dend(hc.complete, k = 4,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

ind4.complete <- cutree(hc.complete, 4)

# Who are in the fourth cluster?
dat[ind4.complete == 4,]
```

To display more details, we show the heatmap of the data.

```{r, fig.width = 12, fig.height=7}
#display.brewer.all(n=NULL, type="all", select=NULL, exact.n=TRUE)
#col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
#col2 <- colorRampPalette(brewer.pal(3, "Spectral"))(2)
#
#heatmap.2(t(dat), 
#          col = col1, keysize=.8, key.par = list(cex=.5),
#          trace = "none", key = TRUE, cexCol = 0.75, 
#          labCol = as.character(dat[,1]),
#          ColSideColors = col2[as.numeric(dat[,"Legendary"])+1],
#          margins = c(10, 10))
```