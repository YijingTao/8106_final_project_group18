---
title: "Midterm Project"
author: "Yijing Tao yt2785"
date: '2022-03-16'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(readxl)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(mgcv)
library(earth)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
library(lasso2)
library(vip)
library(summarytools)
library(ISLR)
library(caret)
library(vip)
library(pdp)
library(lime)
library(mlbench)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer) 
library(gplots)
library(jpeg)
library(klaR)
library(pROC)
```

## import data set (70% to train data while 30% to test data)
```{r}
dat_df_nores = read_csv("./data.csv")%>% 
  na.omit() %>% 
  dplyr::select(-diagnosis,-id)
dat_df_res = read_csv("./data.csv") %>% 
  na.omit() %>% 
  dplyr::select(diagnosis)
dat_df = cbind(dat_df_nores, dat_df_res) 

dat_df2 <- model.matrix(diagnosis ~ ., dat_df)[ ,-1]
#assign 70% to the training set and the rest 30% to the test set 
set.seed(2022)
trainRows <- createDataPartition(dat_df$diagnosis, p = .7, list = F)

train <- dat_df[trainRows,]
x1 <- dat_df2[trainRows,]
train$diagnosis[which(train$diagnosis =='M')] <- 'neg'
train$diagnosis[which(train$diagnosis =='B')] <- 'pos'
train <- train %>% 
  mutate(diagnosis = as.factor(diagnosis))
y1 <- train$diagnosis

# matrix of predictors (glmnet uses input matrix)
test <- dat_df[-trainRows,]
x2 <- dat_df2[-trainRows,]
test$diagnosis[which(test$diagnosis =='M')] <- 'neg'
test$diagnosis[which(test$diagnosis =='B')] <- 'pos'
test <- test %>% 
  mutate(diagnosis = as.factor(diagnosis))
y2 <- test$diagnosis

ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

## visualization
```{r}
dfSummary(dat_df[,-1])
```

```{r, fig.height = 4}
diag <- train[,31]
train_mean <- cbind(train[,1:10],diag)
train_se <- cbind(train[,11:20],diag)
train_worst <- cbind(train[,21:30],diag)
partimat(as.factor(diag) ~ ., 
         data = train_mean, method = "lda")
partimat(as.factor(diag) ~ ., 
         data = train_se, method = "lda")
partimat(as.factor(diag) ~ ., 
         data = train_worst, method = "lda")
```

```{r}
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = train, 
            y = train$diagnosis,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

## glm model
```{r}
contrasts(train$diagnosis)
set.seed(2022)
model.glm <- train(x = train[,1:30],
                   y = train$diagnosis,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

test.pred.prob.glm <- predict(model.glm, newdata = test)
glm_matrix <- confusionMatrix(data = test.pred.prob.glm,
                reference = test$diagnosis,
                positive = "pos")
glm_matrix 
```

## Penalized logistic regression
```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, -1, length = 50)))
set.seed(2022)
model.glmn <- train(x = train[,1:30],
                    y = train$diagnosis,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

model.glmn$bestTune
log(model.glmn$bestTune$lambda)

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))

test.pred.prob.glmn <- predict(model.glmn, newdata = test)
glmn_matrix <- confusionMatrix(data = test.pred.prob.glmn,
                reference = test$diagnosis,
                positive = "pos")
glmn_matrix 
```

## GAM

```{r}
set.seed(2022)
model.gam <- train(x = train[,1:30],
                   y = train$diagnosis,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl)


model.gam$finalModel
log(model.gam$finalModele$lambda)
plot(model.gam$finalModel, select = 3)

test.pred.prob.gam<- predict(model.gam, newdata = test)
gam_matrix <- confusionMatrix(data = test.pred.prob.gam,
                reference = test$diagnosis,
                positive = "pos")
gam_matrix 
```

## MARS

```{r}
set.seed(2022)
model.mars <- train(x = train[,1:30],
                    y = train$diagnosis,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:4, 
                                           nprune = 2:20),
                    metric = "ROC",
                    trControl = ctrl)

plot(model.mars)

coef(model.mars$finalModel) 

pdp::partial(model.mars, pred.var = c("age"), grid.resolution = 200) %>% autoplot()

vip(model.mars$finalModel)

test.pred.prob.mars <- predict(model.mars, newdata = test)
mars_matrix <- confusionMatrix(data = test.pred.prob.mars,
                reference = test$diagnosis,
                positive = "pos")
mars_matrix 
```

## LDA
```{r}
set.seed(2022)
model.lda <- train(x = train[,1:30],
                   y = train$diagnosis,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

test.pred.prob.lda <- predict(model.lda, newdata = test)
lda_matrix <- confusionMatrix(data = test.pred.prob.lda,
                reference = test$diagnosis,
                positive = "pos")
lda_matrix 
```

## QDA

```{r}
set.seed(2022)
model.qda <- train(x = train[,1:30],
                   y = train$diagnosis,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

test.pred.prob.qda <- predict(model.qda, newdata = test)
qda_matrix <- confusionMatrix(data = test.pred.prob.qda,
                reference = test$diagnosis,
                positive = "pos")
qda_matrix 
```

## Naive Bayes (NB)
```{r, warning=FALSE}
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(.2, 3, by = .2))

set.seed(11)
model.nb <- train(x = train[,1:30],
                  y = train$diagnosis,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)

plot(model.nb)
```

## tree
```{r}
set.seed(2022)
rpart.fit <- train(diagnosis ~ . , 
                   train, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

### CIT

```{r}
set.seed(2022)
ctree.fit <- train(diagnosis ~ . , 
                   train, 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 50))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
```

```{r, fig.width=15, fig.height=6}
plot(ctree.fit$finalModel)
summary(resamples(list(rpart.fit, ctree.fit)))
```

### Random forests

```{r}
rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(2022)
rf.fit <- train(diagnosis ~ . , 
                train, 
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```

### AdaBoost

```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)
set.seed(2022)
gbmA.fit <- train(diagnosis ~ . , 
                  train, 
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)

gbmA.pred <- predict(gbmA.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```


```{r}
resamp <- resamples(list(rf = rf.fit, 
                         gbmA = gbmA.fit))
summary(resamp)
```

## linear
```{r}
# kernlab
set.seed(2022)
svml.fit <- train(diabetes ~ . , 
                  data = dat[rowTrain,], 
                  method = "svmLinear",
                  # preProcess = c("center", "scale"),
                  tuneGrid = data.frame(C = exp(seq(-5,2,len=50))),
                  trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)

# e1071
set.seed(2022)
svml.fit2 <- train(diabetes ~ . , 
                  data = dat[rowTrain,], 
                  method = "svmLinear2",
                  tuneGrid = data.frame(cost = exp(seq(-5,2,len=50))),
                  trControl = ctrl)

plot(svml.fit2, highlight = TRUE, xTrans = log)
```

## radial
```{r, fig.width=15, fig.height=8}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=20)),
                         sigma = exp(seq(-6,-2,len=20)))

# tunes over both cost and sigma
set.seed(1)             
svmr.fit <- train(diabetes ~ . , dat, 
                  subset = rowTrain,
                  method = "svmRadialSigma",
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

myCol<- rainbow(20)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar)

# tune over cost and uses a single value of sigma based on kernlab's sigest function
set.seed(1)             
svmr.fit2 <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "svmRadialCost",
                   tuneGrid = data.frame(C = exp(seq(-3,3,len=20))),
                   trControl = ctrl)

# Platt¡¯s probabilistic outputs; use with caution
set.seed(1)             
svmr.fit3 <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "svmRadialCost",
                   tuneGrid = data.frame(C = exp(seq(-3,3,len=20))),
                   trControl = ctrl,
                   prob.model = TRUE) 
# predict(svmr.fit3, newdata = x_test, type = "prob")

set.seed(1)             
rpart.fit <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneLength = 50,
                   trControl = ctrl)
```


### Variable importance
```{r}
set.seed(2022)
vip(rf.fit, 
    method = "permute", 
    train = train,
    target = "Life.expectancy",
    metric = "RMSE",
    nsim = 10,
    pred_wrapper = predict,
    geom = "boxplot", 
    all_permutations = TRUE,
    mapping = aes_string(fill = "Variable")) 
```

### pdp ?????
```{r, fig.width = 8, fig.height = 4}
pdp1.rf <- rf.fit %>% 
  partial(pred.var = c("CRBI")) %>%
  autoplot(train = train, rug = TRUE) 

pdp2.rf <- rf.fit %>% 
  partial(pred.var = c("CRBI","CAtBat"), chull = TRUE) %>%
  autoplot(train = train, rug = TRUE) 

grid.arrange(pdp1.rf, pdp2.rf, nrow = 1)
```

### Individual conditional expectation (ICE) curves ???
```{r, fig.width = 8, fig.height = 4}
ice1.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = expect_df, alpha = .1) +
  ggtitle("ICE, not centered") 

ice2.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = expect_df, alpha = .1, 
           center = TRUE) +
  ggtitle("ICE, centered") 


grid.arrange(ice1.rf, ice2.rf, nrow = 1)
```

### lime ????
```{r, warning=FALSE, fig.height = 10}
explainer.rf <- lime(train[,1:18], rf.fit)

new_obs <- test[,1:18][1:10,]
explanation.obs <- explain(new_obs,
                           explainer.rf, 
                           n_features = 10)

plot_features(explanation.obs)
plot_explanations(explanation.obs)
```


# K means clustering
```{r}
dat <- rbind(train,test)

train_scl <- scale(train[,-1])
test_scl <- scale(test[,-1])
dat_scl <- rbind(train_scl,test_scl)
```


```{r, fig.height=3.5}
fviz_nbclust(dat_scl,
             FUNcluster = kmeans,
             method = "silhouette")

km <- kmeans(dat_scl, centers = 2, nstart = 20)
```

```{r}
km_vis <- fviz_cluster(list(data = dat_scl, cluster = km$cluster), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 

km_vis
```

# Hierarchical clustering
```{r}
hc.complete <- hclust(dist(dat_scl), method = "complete")
```

```{r, fig.width=7}
fviz_dend(hc.complete, k = 4,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

ind4.complete <- cutree(hc.complete, 4)

# Who are in the fourth cluster?
dat[ind4.complete == 4,]
```

To display more details, we show the heatmap of the data.

```{r, fig.width = 12, fig.height=7}
#display.brewer.all(n=NULL, type="all", select=NULL, exact.n=TRUE)
#col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
#col2 <- colorRampPalette(brewer.pal(3, "Spectral"))(2)
#
#heatmap.2(t(dat), 
#          col = col1, keysize=.8, key.par = list(cex=.5),
#          trace = "none", key = TRUE, cexCol = 0.75, 
#          labCol = as.character(dat[,1]),
#          ColSideColors = col2[as.numeric(dat[,"Legendary"])+1],
#          margins = c(10, 10))
```